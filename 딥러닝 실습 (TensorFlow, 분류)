<TensorFlow 분류(Classification) 모델>

- TensorFlow
    - 머신러닝 프레임워크. 다양한 머신러닝과 딥러닝 알고리즘 제공
    - 그래프 기반의 연산 수행

- 활성화 함수
    - 인공 신경망에서 입력 신호의 가중치 합을 출력 신호로 변환하는 함수
    - 종류
        - 시그모이드
        - ReLU
        - 하이퍼볼릭 탄젠트
        - Swish

- Keras
    - 딥러닝 모델을 구축할 수 있는 파이썬 라이브러리

- 모델 컴파일
    - 모델이 학습하기 위해 필요한 손실 함수(loss function), 최적화 알고리즘(optimizer) 및 평가 지표 (metrics)를 지정하는 작업
        - 손실함수
            - 모델이 예측한 출력값과 실제 정답값의 차이를 계산하는 함수
            - 이진 분류 → binary_crossentropy
            - 다중 클래스 분류 → categorical_crossentropy
        - 최적화 알고리즘 (Optimizer)
            - 손실함수를 최소화하는 방향으로 모델 파라미터를 업데이트하는 알고리즘
            - SGD, Adam
        - 평가 지표 (metircs)
            - 모델의 성능을 측정하는 지표
            - 정확도, 정밀도, 재현율, F1-score 등 계산

- 라이브러리

```python
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
```

- 데이터

```python
# 데이터셋을 불러옵니다.
df = pd.read_csv("https://bit.ly/telco-csv", index_col="customerID")
df.shape
```

```python
# 미리보기를 합니다.
df.head()
```

```python
# 히스토그램으로 시각화 합니다.
df.hist();
```

```python
# describe 로 요약합니다.
df.describe()
```

- 전처리

```python
# TotalCharges 가 수치 타입이 아니기 때문에 수치 연산을 위해 숫자 형태로 변경합니다.
df["TotalCharges"] = pd.to_numeric(df["TotalCharges"], errors="coerce"
```

```python
# 결측치 제거
df = df.dropna()
```

```python
# 결측치 제거 확인
df.shape
```

- 학습, 예측 데이터셋 나누기

```python
# label_name
label_name = "Churn"
```

```python
# X, y 만들기
X = pd.get_dummies(df.drop(columns=label_name))
y = (df[label_name] == "Yes").astype(float)

X.shape, y.shape
```

```python
# sklearn.model_selection 으로 데이터셋 나누기
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, stratify=y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape, y_train.shape, y_test.shape
```

- TensorFlow

```
# tensorflow 를 tf로 불러오기
import tensorflow as tf
```

- 활성화 함수

```python
print(dir(tf.keras.activations))
```

1) Sigmoid

```python
# tf.keras.activations.sigmoid(x)
# x축은 원래 값을 y축은 sigmoid 함수를 통과시킨 값입니다. 

plt.plot(x, tf.keras.activations.sigmoid(x), linestyle='--') 
plt.title("sigmoid")
plt.axvline(0, alpha=0.2)
plt.axhline(0.5, alpha=0.2)
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5150fdf2-bb9d-4cc5-a85d-12f14d3698ce/Untitled.png)

```python
# tanh

plt.plot(x, tf.keras.activations.tanh(x), linestyle='--') 
plt.title("tanh")
plt.axvline(0, alpha=0.2)
plt.axhline(0, alpha=0.2)
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/9d13f320-2667-4468-ac59-a4c75fb71f3b/Untitled.png)

```python
# swish

plt.plot(x, tf.keras.activations.swish(x), linestyle='--') 
plt.title("swish")
plt.axvline(0, alpha=0.2)
plt.axhline(0, alpha=0.2)
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/83d8c47e-fd25-410c-ab74-808af1f44bf8/Untitled.png)

```python
# relu

plt.plot(x, tf.keras.activations.relu(x), linestyle='--') 
plt.title("relu")
plt.axvline(0, alpha=0.2)
plt.axhline(0, alpha=0.2)
```

![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/22d0340a-c332-4881-9bef-267edd042e96/Untitled.png)

- 딥러닝 레이어 만들기

*  층 설정
    * 신경망의 기본 구성 요소는 층(layer)입니다. 
    * 층은 주입된 데이터에서 표현을 추출합니다. 
    * 대부분 딥러닝은 간단한 층을 연결하여 구성됩니다. 
    * tf.keras.layers.Dense와 같은 층들의 가중치(parameter)는 훈련하는 동안 학습됩니다.

* Dense Layer: 
    * 밀집 연결(densely-connected) 또는 완전 연결(fully-connected) 층이라고 부릅니다. 
    * 첫 번째 Dense 층은 128개의 노드(또는 뉴런)를 가집니다. 
    * 마지막 층은 출력층 입니다.
        * 소프트맥스 일 때 : 2개의 노드의 소프트맥스(softmax) 층입니다. 이 층은 2개의 확률을 반환하고 반환된 값의 전체 합은 1입니다. 
        * 각 노드는 현재 이미지가 2개 클래스 중 하나에 속할 확률을 출력합니다.
        * 시그모이드 일 때 : 둘 중 하나를 예측할 때 1개의 출력값을 출력합니다. 확률을 받아 임계값 기준으로 True, False로 나눕니다.

```python
# 입력데이터 수 구하기
input_shape = X_train.shape[1]
input_shape
```

```python
# tf.keras.models.Sequential 로 입력-히든-출력(sigmoid) 레이어로 구성
model = tf.keras.Sequential([
    tf.keras.layers.Dense(64, activation='relu', 
                          input_shape=[input_shape]),
    tf.keras.layers.Dense(32, activation='relu'),
    tf.keras.layers.Dense(1, activation='sigmoid')
  ])

model.summary()
```

```python
# Param 값 계산하기
(input_shape * 64)  + 64
```

- 모델 컴파일

```python
tf.keras.losses.BinaryCrossentropy()
```

```python
# 모델 컴파일
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=optimizer, 
              loss="binary_crossentropy",
              metrics=["accuracy"]
              )
```

- 학습

```python
class PrintDot(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs):
        if epoch % 100 == 0: print('')
        print('.', end='')

# val_loss 기준으로 값이 나아지지 않으면 멈추게 합니다.
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
                                              patience=50,)
```

```python
# 학습하기
# callbacks=[early_stop, PrintDot()]
history = model.fit(X_train, y_train, validation_split=0.2, 
                    epochs=1000, 
                    callbacks=[early_stop, PrintDot()], verbose=0)
history
```

```python
# 학습결과의 history 값을 가져와서 비교하기 위해 데이터프레임으로 변환
df_hist = pd.DataFrame(history.history)
df_hist.tail()
```

- 학습 결과 시각화

```python
# loss, accuracy 값 시각화 
fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))
df_hist[["loss", "val_loss"]].plot(ax=axes[0])
df_hist[["accuracy", "val_accuracy"]].plot(ax=axes[1])
```

- 예측

```python
# 예측값을 y_pred 변수에 할당 후 재사용합니다.
# (질문) 바이너리이면 결과가 2개 나와야 하는거 아닌가요? => 소프트맥스라면 2개가 나옵니다.
# 바이너리로 했기 때문에 예측값이 각 row 당 예측값이 하나씩 나옵니다.
# 여기에서 나오는 값은 확률값(sigmoid) 0~1 사이의 값을 갖습니다.
# 특정 임계값을 정해서 크고 작다를 통해 True, False값으로 판단합니다.
# 임계값은 보통 0.5 를 사용하지만 다른 값을 사용하기도 합니다.
y_pred = model.predict(X_test)
y_pred[:5], y_pred.shape
```

```python
# 예측값 시각화
# 임계값을 정해서 특정값 이상이면 True, 아니면 False로 변환해서 사용할 예정입니다.
# flatten 은 어제 MNIST, FMNIST 에서 입력값을 만들때 사용했던 Flatten Layer를 떠올려 보세요.
y_predict = y_pred.flatten()
y_predict[:5], y_predict.min(), y_predict.max()
```

- 평가

```python
# test_loss, test_acc
test_loss, test_acc = model.evaluate(X_test, y_test)
test_loss, test_acc
```

```python
# argmax는 다차원 배열의 차원에 따라 가장 큰 값의 인덱스들을 반환해주는 함수
# 출력층에서 softmax 사용 시 확률로 반환되기 때문에 그 중 가장 높은 값의 인덱스를 찾을 때 사용
# y_predict = np.argmax(y_pred, axis=1)
# 출력층 sigmoid 사용 시 flatten 을 통해 예측값을 1차원으로 변환
y_predict = y_pred.flatten() > 0.5
y_predict[:5]
```

```python
# 직접 정확도 측정하기
(y_test == y_predict).mean()
```
